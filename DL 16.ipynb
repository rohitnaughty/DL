{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6276a9-e8fc-4db8-bfe2-7227b2a3b801",
   "metadata": {},
   "source": [
    "1. Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "b) tanh\n",
    "c) ReLU\n",
    "d) ELU\n",
    "e) LeakyReLU\n",
    "f) swish\n",
    "Sigmoid(x) = 1/1+e^-x\n",
    "\n",
    "➡️If big x sigmoid near to 1 else 0\n",
    "\n",
    "➡️Advantage:-\n",
    "\n",
    "➡️Range between 0 and 1 so we can use in output layer as probability\n",
    "\n",
    "➡️Non linear function\n",
    "\n",
    "➡️Diffrentiable\n",
    "\n",
    "➡️Disadvantages:-\n",
    "\n",
    "➡️Saturated Function because of range and its become vanishing gradient problem.\n",
    "\n",
    "➡️Non zero centered but its done by batch normalization but training get slow.\n",
    "\n",
    "➡️Computationally Expensive because of exponent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed08cc3-503b-4983-bece-22d2cb9ad5ce",
   "metadata": {},
   "source": [
    "TanH\n",
    "\n",
    "➡️Range -1 to 1\n",
    "\n",
    "➡️Formula:- (e^x -e^-x)/(e^x+e^-x)\n",
    "\n",
    "➡️Advantages:-\n",
    "\n",
    "➡️Non linear\n",
    "\n",
    "➡️Diffrentiable\n",
    "\n",
    "➡️Zero centered\n",
    "\n",
    "➡️Disadvantages:-\n",
    "\n",
    "➡️Saturating Function\n",
    "\n",
    "➡️Computationally Expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae8394-51c9-4152-8d32-79c9f990431f",
   "metadata": {},
   "source": [
    "RELU\n",
    "\n",
    "➡️Range -1 to 1\n",
    "\n",
    "➡️Formula:- (e^x -e^-x)/(e^x+e^-x)\n",
    "\n",
    "➡️Advantages:-\n",
    "\n",
    "➡️Non linear\n",
    "\n",
    "➡️Diffrentiable\n",
    "\n",
    "➡️Zero centered\n",
    "\n",
    "➡️Disadvantages:-\n",
    "\n",
    "➡️Saturating Function\n",
    "\n",
    "➡️Computationally Expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df5b13-ec32-4bb3-9ba8-8a3d4dc4f7ab",
   "metadata": {},
   "source": [
    "ELU (Exponential Linear Unit) ➡️Formula elu(x):- max(alpha(e^x -1),x)\n",
    "\n",
    "➡️Advantage:-\n",
    "\n",
    "➡️Non Saturated\n",
    "\n",
    "➡️Easily Computationally\n",
    "\n",
    "➡️Close to zero centerd or convergence faster\n",
    "\n",
    "➡️flexiblity because of trainable paramter\n",
    "\n",
    "➡️No dying relu\n",
    "\n",
    "➡️Genralization is better\n",
    "\n",
    "➡️Always Continious as well as Diffrentiable\n",
    "\n",
    "➡️Disadvantages:- Computational expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af8d4f-cb91-46cc-b154-62ede3430223",
   "metadata": {},
   "source": [
    "Leaky Relu ➡️Formula:- max(0.0.1*z,z)\n",
    "\n",
    "➡️z>0 -> z else 0.01*z(its solve Diffrentiable problem because of output is not 0)\n",
    "\n",
    "➡️Advantage:-\n",
    "\n",
    "➡️Non Saturated\n",
    "\n",
    "➡️Easily Computationally\n",
    "\n",
    "➡️Close to zero centerd\n",
    "\n",
    "➡️No dying relu\n",
    "\n",
    "Swish is an activation function, f ( x ) = x ⋅ sigmoid ( β x ) , where a learnable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c2f34-c8b5-40d9-b5b9-4d27426ae4a1",
   "metadata": {},
   "source": [
    "2. What happens when you increase or decrease the optimizer learning rate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97618f3d-baf3-49cc-bbdb-e70a3a08ab51",
   "metadata": {},
   "source": [
    "A smaller learning rate may allow the model to learn a more optimal. If the learning rate is very large we will skip the \n",
    "optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95ea94-9265-4690-bfd7-36cd78f7ac3a",
   "metadata": {},
   "source": [
    "3. What happens when you increase the number of internal hidden neurons?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641cd68-2e09-4ba5-8acb-8f89e318cc18",
   "metadata": {},
   "source": [
    "increase the time it takes to train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540fb1e-ef06-46fc-b530-39b70c0a3108",
   "metadata": {},
   "source": [
    "4. What happens when you increase the size of batch computation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de22de1-c86b-4dbb-bebe-b64237dea626",
   "metadata": {},
   "source": [
    "Reduce the learners \"capacity to generalize\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6448b-d1fe-44b4-a04a-9ea46da70fc8",
   "metadata": {},
   "source": [
    "5. Why we adopt regularization to avoid overfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf783c0-6771-4cbd-89fa-847ed8aa4a5c",
   "metadata": {},
   "source": [
    "Regularization assumes that least weight may produce simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cd86d-f970-4cab-9b01-d109f888a96a",
   "metadata": {},
   "source": [
    "6. What are loss and cost functions in deep learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d451ce-b86b-439d-a3c5-a7898289faca",
   "metadata": {},
   "source": [
    "Loss function is for a line and cost function is for whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab446be-5a0a-4fa2-96db-939f381f28e1",
   "metadata": {},
   "source": [
    "7. What do ou mean by underfitting in neural networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ad7fa-b896-4d59-be4c-84545d3d2706",
   "metadata": {},
   "source": [
    "Underfitting is a scenario in data science where a data model is unable to capture the relatioship between the \n",
    "input and output variable accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2ad6f-91fd-4a15-8eff-7159043c4f9f",
   "metadata": {},
   "source": [
    "8. Why we use Dropout in Neural Networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce693a9-279f-46b1-95f4-90020372b768",
   "metadata": {},
   "source": [
    "Reduce Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577cdbe-dd58-465c-9d67-ac7acd7b88bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
