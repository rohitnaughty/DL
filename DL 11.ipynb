{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655bb166-f057-44a4-a329-e844e767c436",
   "metadata": {},
   "source": [
    "1. Write the Python code to implement a single neuron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae97f1a-5835-4a1f-90dc-bb10e2b1be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs):\n",
    "        self.weights = np.random.randn(num_inputs)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return np.dot(inputs, self.weights) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c392410-600e-4480-9e22-7f1ea79f3f75",
   "metadata": {},
   "source": [
    "2. Write the Python code to implement ReLU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5223e0a-db8d-456d-9fcd-89a69b72bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e4de5-677f-45b8-a995-d9e2ede25648",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Write the Python code for a dense layer in terms of matrix multiplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53bb577-e3af-4128-b67c-efb55c12b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return np.dot(X, self.weights) + self.biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d9758-f86f-4654-abb8-48a728c89535",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
    "and functionality built into Python).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dae1bf0-95ab-480a-aa30-cde59c61acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(inputs, weights, biases):\n",
    "    \"\"\"Compute the output of a dense layer using matrix multiplication.\"\"\"\n",
    "    # Matrix multiplication\n",
    "    outputs = [[sum(x * y for x, y in zip(row, col)) for col in zip(*weights)] for row in inputs]\n",
    "    # Add biases\n",
    "    outputs = [[x + b for x, b in zip(row, biases)] for row in outputs]\n",
    "    # Apply activation function (if any)\n",
    "    outputs = [[max(0, x) for x in row] for row in outputs]  # ReLU activation function\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7cc6b-134c-452a-b768-03ff8d1ecd8b",
   "metadata": {},
   "source": [
    "5. What is the “hidden size” of a layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486298e-7561-4429-99c3-36d53cf30f8a",
   "metadata": {},
   "source": [
    "The \"hidden size\" of a layer refers to the number of neurons in that layer. It is called \"hidden\" because it is not part of the input or output layer and is not directly observable. Instead, the hidden layer processes the input data and passes it on to the output layer, which produces the final output. The hidden size is a hyperparameter that can be adjusted during model design and training to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474588a5-87c3-4f18-a94a-e838c3c1225d",
   "metadata": {},
   "source": [
    "6. What does the t method do in PyTorch?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef4b082c-b14c-4b39-9cd9-7ce7fc2d3a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.7)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.0)\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit\n",
      "  Downloading lit-16.0.2.tar.gz (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.2-py3-none-any.whl size=88177 sha256=ab711b09aeb3958671a047cee1b1f328453fa5392da49f1e79ca87f9db345912\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e2/ba/ec/aad60e5609dfac997d2902a02912124867d56a41334570a4e4\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.3 filelock-3.12.0 lit-16.0.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdecbbb6-bc9d-4c7c-a726-732d73d9f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x.t()) # tensor([[1, 4], [2, 5], [3, 6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a3b3d-9c4a-4263-9094-cf77bb899e16",
   "metadata": {},
   "source": [
    "7. Why is matrix multiplication written in plain Python very slow?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa673535-0bb4-4290-a50a-1691feecf622",
   "metadata": {},
   "source": [
    "Matrix multiplication written in plain Python is slow because it involves iterating through nested loops to perform the multiplication of each element of the matrices, resulting in a time complexity of O(n^3), where n is the size of the matrix. This is not efficient for large matrices, as the number of operations required grows very quickly. On the other hand, optimized libraries like BLAS (Basic Linear Algebra Subprograms) and cuBLAS (CUDA Basic Linear Algebra Subprograms) use highly optimized algorithms and parallel processing to perform matrix multiplication much faster than plain Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eeffff-0a37-42b9-bd7b-59cfb7498d94",
   "metadata": {},
   "source": [
    "8. In matmul, why is ac==br?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af6ddb5-cefe-4579-b676-9f917da5d062",
   "metadata": {},
   "source": [
    "In matmul, the condition `ac == br` is required for matrix multiplication to be defined mathematically. \n",
    "\n",
    "If we multiply a matrix A of size `(a, b)` with a matrix B of size `(c, d)`, then we can only perform the multiplication if the inner dimensions of the matrices match, that is, if `b == c`. In this case, the resulting matrix will have size `(a, d)`. \n",
    "\n",
    "So in the condition `ac == br`, `ac` represents the product of the number of rows of the first matrix and the number of columns of the second matrix, and `br` represents the product of the number of columns of the first matrix and the number of rows of the second matrix. If these two products are equal, then the matrices can be multiplied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db7f91-9b0c-4c8b-9f6b-4f9475936b45",
   "metadata": {},
   "source": [
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6db2d2-3650-420a-8a6d-c95fc0a33078",
   "metadata": {},
   "source": [
    "You can use the magic command %timeit at the beginning of the cell to measure the time taken to execute the code inside the cell. The command will run the code a few times to get an accurate measurement of the average execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7013de4-d1e3-46bc-a8c1-3ee79d817c5d",
   "metadata": {},
   "source": [
    "10. What is elementwise arithmetic?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ba2fc-f1c9-4e5e-875e-c90a7d0ec042",
   "metadata": {},
   "source": [
    "Elementwise arithmetic refers to performing arithmetic operations between two matrices or tensors by applying the operation between the corresponding elements of the matrices or tensors. For example, given two matrices A and B, elementwise addition would result in a new matrix C, where each element of C is the sum of the corresponding elements of A and B. Similarly, elementwise multiplication would result in a new matrix D, where each element of D is the product of the corresponding elements of A and B. This operation is also known as Hadamard product or Schur product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c6337-aae3-4613-8285-22265ea6dc76",
   "metadata": {},
   "source": [
    "11. Write the PyTorch code to test whether every element of a is greater than the\n",
    "corresponding element of b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659aff44-79ab-49da-9093-964f3d36b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False,  True])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 2])\n",
    "\n",
    "greater_than_b = torch.gt(a, b)\n",
    "\n",
    "print(greater_than_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128deebf-632e-43e1-bd5b-83ea0e82f3a2",
   "metadata": {},
   "source": [
    "12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237886d5-b41a-4d64-b57a-24b06d41cc00",
   "metadata": {},
   "source": [
    "A rank-0 tensor is a tensor with no dimensions, also called a scalar. In PyTorch, it is represented as a tensor with an empty shape, i.e., `torch.tensor(42)` would be a rank-0 tensor representing the scalar value 42.\n",
    "\n",
    "To convert a rank-0 tensor to a plain Python data type, you can use the `item()` method, like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b480223-e20a-44d0-8f32-1fec90dc6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(42)\n",
    "y = x.item()  # y is now the Python integer 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd5888-27ee-483b-a6d7-240749bc6b1b",
   "metadata": {},
   "source": [
    "13. How does elementwise arithmetic help us speed up matmul?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664f910-2c30-4476-b2d3-dc8cd1fcd461",
   "metadata": {},
   "source": [
    "Elementwise arithmetic helps us speed up matmul by allowing us to perform the multiplication of corresponding elements in two tensors, rather than performing the full matrix multiplication. This can be done much more efficiently using vectorized operations in hardware, such as GPUs. Once we have performed the elementwise multiplication, we can sum the resulting tensors along appropriate axes to get the final result of the matrix multiplication. This approach can be much faster than performing the full matrix multiplication, especially for large matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da042e-3c30-4628-98f2-2285032cf940",
   "metadata": {},
   "source": [
    "14. What are the broadcasting rules?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba862fc-2331-4f7c-8c4e-b047cd673cdc",
   "metadata": {},
   "source": [
    "Broadcasting is a feature in NumPy and PyTorch that allows arrays of different shapes to be used in arithmetic operations. The broadcasting rules are as follows:\n",
    "\n",
    "1. If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side.\n",
    "2. If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.\n",
    "3. If in any dimension the sizes disagree and neither is equal to 1, an error is raised.\n",
    "\n",
    "For example, if we have two arrays `a` and `b`, where `a` is of shape `(3, 1)` and `b` is of shape `(1, 4)`, we can add them together as follows:\n",
    "\n",
    "```\n",
    "a = np.array([[1], [2], [3]])\n",
    "b = np.array([[4, 5, 6, 7]])\n",
    "\n",
    "c = a + b\n",
    "\n",
    "# c is now:\n",
    "# array([[5, 6, 7, 8],\n",
    "#        [6, 7, 8, 9],\n",
    "#        [7, 8, 9, 10]])\n",
    "```\n",
    "\n",
    "Here, the shape of `a` is padded with a 1 on its left to become `(3, 1)`, and the shape of `b` is padded with a 1 on its left to become `(1, 4)`. Then, both arrays are broadcast to shape `(3, 4)` by stretching the size-1 dimensions, and the addition is performed elementwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b8ab1-3816-47b3-bec4-bd1b472e6f97",
   "metadata": {},
   "source": [
    "15. What is expand_as? Show an example of how it can be used to match the results of\n",
    "broadcasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e59782-46bf-4eee-8a47-34fc0e3eeefa",
   "metadata": {},
   "source": [
    "\n",
    "In PyTorch, expand_as is a method that returns a new tensor with the same data as the input tensor, but with the specified shape. It is used to match the shapes of two tensors so that broadcasting can be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8daad7-d1c4-4a36-9af3-25fc2dc3823c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
