{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afab1ac-5873-4329-b688-04a7ddfe8190",
   "metadata": {},
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7ba0e-9465-43bc-8117-05b35c90f3c7",
   "metadata": {},
   "source": [
    "Yes, it is generally okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization. The key idea behind He initialization is to set the initial weights to random values that take into account the size of the layer and the activation function used. By using a random initialization, we avoid having all the weights initialized to the same value, which can cause problems such as symmetrical weights and gradient descent being stuck at a certain point.\n",
    "\n",
    "However, initializing all weights to the same value (even if that value is randomly generated) may still result in poor performance of the network due to symmetry issues. Therefore, it is generally better to use a proper initialization scheme like He initialization that takes into account the size of the layer and activation function used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647d47d-2552-48a4-83c0-6977064019a5",
   "metadata": {},
   "source": [
    "2. Is it OK to initialize the bias terms to 0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ad0b8-86a6-4179-876f-d0a9835dfc16",
   "metadata": {},
   "source": [
    "Yes, it is generally okay to initialize the bias terms to 0. This is a common practice in deep learning, and many popular frameworks, such as TensorFlow and PyTorch, use this default initialization for biases. The reason for this is that the initial bias values do not significantly affect the training of the neural network, as the gradient of the bias term is not directly influenced by the input data.\n",
    "\n",
    "However, some studies have shown that initializing the bias terms to small positive values, such as 0.1, can help improve the convergence speed of the network, especially in networks with ReLU activation functions. This is because a small positive bias can help ensure that neurons in the network start firing and learning from the very beginning of the training process. So, while it is generally okay to initialize the bias terms to 0, initializing them to small positive values may provide some benefits in certain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c4901-f338-4731-8767-b687bd417066",
   "metadata": {},
   "source": [
    "3. Name three advantages of the SELU activation function over ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc734c-f1f4-4ba3-bf59-20eef41aeded",
   "metadata": {},
   "source": [
    "Here are three advantages of the SELU activation function over ReLU:\n",
    "\n",
    "Avoids the dying ReLU problem: One of the main disadvantages of the ReLU activation function is that it can lead to a large number of \"dead\" neurons that never activate, a problem known as the \"dying ReLU\" problem. The SELU activation function avoids this problem by having a negative slope for negative inputs, which can activate neurons that would otherwise be \"dead\" with ReLU.\n",
    "\n",
    "Self-normalizing: The SELU activation function was designed to have a mean activation of 0 and a standard deviation of 1 for inputs in the normal distribution. This means that deep neural networks using SELU can self-normalize, which can result in faster convergence and better performance.\n",
    "\n",
    "Improved performance: Several studies have shown that neural networks using SELU activation functions can achieve better performance than those using ReLU, especially for deep neural networks with many layers. This is due to the self-normalizing property of the SELU function, as well as its avoidance of the dying ReLU problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f732f2-83ca-461a-bfb1-868ae8a8298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919d506-f383-42d4-9c9d-ee9198952fe6",
   "metadata": {},
   "source": [
    "- SELU: It is recommended to use SELU activation function for deep neural networks, particularly feedforward neural networks, as it ensures that the weights are scaled in such a way that the output of each layer will have zero mean and unit variance, which leads to self-normalization and improved convergence. However, SELU should only be used with dense layers, as the normalization property is lost in convolutional layers.\n",
    "\n",
    "- Leaky ReLU (and its variants): Leaky ReLU can be used as a replacement for ReLU when you want to avoid \"dying ReLU\" problem, which occurs when a large fraction of neurons in a layer output zero due to a very large negative bias. Leaky ReLU solves this problem by introducing a small slope (typically 0.01) for negative inputs. Variants of Leaky ReLU include Parametric ReLU (PReLU), which allows the slope to be learned during training, and Exponential Linear Units (ELU), which have similar advantages to SELU.\n",
    "\n",
    "- ReLU: ReLU should be used as the default choice for activation function, as it is simple, computationally efficient, and performs well in most cases. ReLU is especially useful in deep neural networks, where it can help alleviate the vanishing gradient problem.\n",
    "\n",
    "- tanh: tanh can be used as an alternative to ReLU, especially when you need outputs between -1 and 1, such as in a binary classification problem. tanh is also useful in recurrent neural networks (RNNs), where it can help avoid the exploding gradient problem.\n",
    "\n",
    "- logistic: logistic (also known as sigmoid) is commonly used as an activation function for binary classification problems, as it outputs values between 0 and 1, which can be interpreted as probabilities. However, it is generally not recommended to use logistic for hidden layers, as it can lead to the vanishing gradient problem.\n",
    "\n",
    "- softmax: softmax is typically used as the activation function for the output layer in multi-class classification problems, as it outputs a probability distribution over the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eac8a9-52b0-49be-8e9e-bf4533a4bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using an SGD optimizer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a94b7-ee3e-4a57-b761-b61acc4027aa",
   "metadata": {},
   "source": [
    "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer can cause the optimizer to overshoot the minimum of the cost function and oscillate around it, rather than converge to it. This is because the momentum term accumulates a lot of past gradients, making the optimizer take large steps in the direction of the minimum, which can cause it to overshoot and oscillate. This can lead to slow convergence or even prevent convergence altogether. Therefore, it is important to choose an appropriate value for the momentum hyperparameter to ensure the optimizer converges to the minimum of the cost function.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a729a42-ca37-45ba-82b4-d16c9d91f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Name three ways you can produce a sparse model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46243273-b90c-4390-b3b0-b1faedc5b6c4",
   "metadata": {},
   "source": [
    "Here are three ways to produce a sparse model:\n",
    "\n",
    "1. L1 regularization: By adding an L1 penalty term to the cost function, the optimizer is encouraged to set the weights of many features to zero, resulting in a sparse model.\n",
    "2. Dropout regularization: During training, dropout randomly drops out some neurons, forcing the network to rely on a smaller subset of features. This can lead to a sparse model.\n",
    "3. Data pruning: After training, weights that are close to zero are set to exactly zero, effectively removing the corresponding connections in the network. This can be done in a one-time post-processing step or by dynamically pruning during training based on certain criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbaee8-55e1-4e21-82b1-0b7b8a6e1c4b",
   "metadata": {},
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)? What about MC Dropout?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df379ae-a9d2-42d9-8fa6-82fd8d5728b7",
   "metadata": {},
   "source": [
    "Yes, dropout does slow down training because it introduces additional computations during each forward and backward pass of the network. During training, dropout randomly sets a fraction of the neurons to zero, and then scales up the remaining neurons to maintain the expected value of each neuron's output. This scaling operation is what causes the additional computations.\n",
    "\n",
    "However, dropout does not slow down inference since it is only applied during training. Inference is simply a forward pass through the network without any dropout applied.\n",
    "\n",
    "MC Dropout, on the other hand, can slow down inference since it requires multiple forward passes with dropout applied to each pass. During each forward pass, dropout randomly sets a fraction of the neurons to zero, and the predictions from all the passes are averaged to produce the final prediction. This can be computationally expensive, especially for large networks and large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2bb2bd-d0b4-4bdf-8e6c-58e3fd0a38fd",
   "metadata": {},
   "source": [
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
    "point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "Remember to search for the right learning rate each time you change the model’s\n",
    "architecture or hyperparameters.\n",
    "\n",
    "c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it affect\n",
    "training speed?\n",
    "\n",
    "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
    "layers, etc.).\n",
    "\n",
    "e. Try regularizing the model with alpha dropout. Then, without retraining your"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bf3827-ca84-4842-86ae-1a639568b1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.8)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.8.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.4)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.9.3)\n",
      "Collecting ml-dtypes>=0.0.3\n",
      "  Downloading ml_dtypes-0.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.2-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.8-py3-none-any.whl size=1439678 sha256=8f8a748253d3f57857f7c9e8cf04a88fb0b4b5a7f7083b771a8ab48df0423d20\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/d6/f6/d5/63686989c723075de411cbc630ca12f4241a8436e411e38d6a\n",
      "Successfully built jax\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 jax-0.4.8 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.2 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.2 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbabed6b-0d06-4d6b-94a1-c24a50ad7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "# Add hidden layers\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "\n",
    "# Add output layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f3ec8-0359-4b57-92e8-1845e549da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 19s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 39s 21ms/step - loss: 2.0072 - accuracy: 0.2718 - val_loss: 1.8048 - val_accuracy: 0.3550\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.8191 - accuracy: 0.3373 - val_loss: 1.7750 - val_accuracy: 0.3655\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.7533 - accuracy: 0.3663 - val_loss: 1.7314 - val_accuracy: 0.3858\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.7181 - accuracy: 0.3808 - val_loss: 1.7057 - val_accuracy: 0.3869\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 1.6850 - accuracy: 0.3952 - val_loss: 1.6856 - val_accuracy: 0.4057\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.6526 - accuracy: 0.4058 - val_loss: 1.6533 - val_accuracy: 0.4090\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.8778 - accuracy: 0.2887 - val_loss: 1.8607 - val_accuracy: 0.2928\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 2.1632 - accuracy: 0.2377 - val_loss: 1.8866 - val_accuracy: 0.2852\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 1.9809 - accuracy: 0.2463 - val_loss: 1.9828 - val_accuracy: 0.2509\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 1.8956 - accuracy: 0.2647 - val_loss: 1.8745 - val_accuracy: 0.2737\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.8505 - accuracy: 0.2838 - val_loss: 1.8579 - val_accuracy: 0.2996\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.8159 - accuracy: 0.3066 - val_loss: 1.8699 - val_accuracy: 0.2892\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.7700 - accuracy: 0.3313 - val_loss: 1.7553 - val_accuracy: 0.3421\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 1.7573 - accuracy: 0.3442 - val_loss: 1.7854 - val_accuracy: 0.3216\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.6886 - accuracy: 0.3754 - val_loss: 1.6936 - val_accuracy: 0.3821\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.6563 - accuracy: 0.3923 - val_loss: 1.7219 - val_accuracy: 0.3598\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 1.6312 - accuracy: 0.4051 - val_loss: 1.6827 - val_accuracy: 0.3964\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.7249 - accuracy: 0.3751 - val_loss: 1.8067 - val_accuracy: 0.3458\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.6762 - accuracy: 0.3896 - val_loss: 1.6916 - val_accuracy: 0.3892\n",
      "Epoch 20/100\n",
      " 197/1250 [===>..........................] - ETA: 18s - loss: 1.6480 - accuracy: 0.3969"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Scale pixel values to [0, 1]\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Nadam(lr=5e-5), metrics=[\"accuracy\"])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.2, callbacks=[early_stopping_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbb701-2ed0-450d-b12e-35dfcd4cba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "# Add hidden layers with Batch Normalization\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "\n",
    "# Add output layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3d3f3-0eac-4e5f-a923-ec2b9c8ff5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(Input(shape=(32, 32, 3)))\n",
    "\n",
    "# Add hidden layers with SELU and LeCun normal initialization\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation=\"selu\", kernel_initializer=lecun_normal()))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Nadam(lr=5e-5), metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_full, y_train_full, epochs=100, validation_split=0.2, callbacks=[early_stopping_cb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59ab54-8eef-471a-8cae-a6461e8117cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "# Add hidden layers with alpha dropout\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "\n",
    "# Add output layer\n",
    "model.add(keras.layers.Dense(10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c4bac-ab8c-4027-98f0-08ac5a82d729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213623ae-910b-4422-abad-01447d74e46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073dda6-1865-4171-9355-0185d5e281d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe6da03-8b1d-4eab-a1ad-65213911aac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
