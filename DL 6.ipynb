{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ef5a34-ed77-4590-976d-951642df61b6",
   "metadata": {},
   "source": [
    "1. What are the advantages of a CNN over a fully connected DNN for image classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3c7cc-f84d-4297-978a-79712ac3d168",
   "metadata": {},
   "source": [
    "CNNs have several advantages over fully connected DNNs for image classification:\n",
    "\n",
    "1. Parameter sharing: CNNs use the same set of weights for each patch of an image, which reduces the number of parameters needed in the model. This allows for a better use of training data and can prevent overfitting.\n",
    "\n",
    "2. Translation invariance: CNNs can recognize an object in an image, regardless of its position within the image. This is achieved by using pooling layers to downsample the image and extract features that are invariant to small translations.\n",
    "\n",
    "3. Hierarchical feature learning: CNNs can learn hierarchical features from the input image, starting with low-level features such as edges and gradually building up to more complex features such as object parts and whole objects.\n",
    "\n",
    "4. Efficiency: CNNs can be trained efficiently using GPUs, making it possible to train large models on large datasets in a reasonable amount of time.\n",
    "\n",
    "Overall, CNNs are a powerful and effective approach for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ced6ae-b7c3-4745-b277-30d86ec50c0b",
   "metadata": {},
   "source": [
    "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of\n",
    "2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs\n",
    "200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much\n",
    "RAM will this network require when making a prediction for a single instance? What about when\n",
    "training on a mini-batch of 50 images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c6302-d522-4a5a-9165-0fc4da8e4f95",
   "metadata": {},
   "source": [
    "To calculate the number of parameters in the CNN, we need to count the number of weights in each layer, including the biases. For a convolutional layer with `k` filters, a kernel size of `n x n`, and an input with `d` channels, the number of parameters is `(n * n * d + 1) * k`.\n",
    "\n",
    "For the first convolutional layer, the input has 3 channels (RGB), the number of filters is 100, and the kernel size is 3x3, so the number of parameters is `(3*3*3+1)*100=2,800`.\n",
    "\n",
    "For the second convolutional layer, the input has 100 channels (output from the first layer), the number of filters is 200, and the kernel size is 3x3, so the number of parameters is `(3*3*100+1)*200=180,200`.\n",
    "\n",
    "For the third convolutional layer, the input has 200 channels (output from the second layer), the number of filters is 400, and the kernel size is 3x3, so the number of parameters is `(3*3*200+1)*400=720,400`.\n",
    "\n",
    "Therefore, the total number of parameters in the CNN is `2,800 + 180,200 + 720,400 = 903,400`.\n",
    "\n",
    "When making a prediction for a single instance, the network will need to store the activations of each layer. The output of the first convolutional layer will be `100 x 100 x 150`, the output of the second layer will be `50 x 50 x 200`, and the output of the third layer will be `25 x 25 x 400`. Each of these activations will require `4 bytes` per activation (assuming we are using 32-bit floats). Therefore, the total memory required for a single prediction will be `100 x 100 x 150 x 4 + 50 x 50 x 200 x 4 + 25 x 25 x 400 x 4 = 78,000,000 bytes = 74.2 MB`.\n",
    "\n",
    "When training on a mini-batch of 50 images, we will need to store the activations and gradients for each layer for each image in the batch. Assuming that we are using backpropagation through the entire network, the total memory required for each layer will be `2 x (batch_size x height x width x num_filters x 4 bytes)`. Therefore, the total memory required for the entire network will be `2 x (50 x 100 x 150 x 100 x 4 + 50 x 50 x 200 x 200 x 4 + 50 x 25 x 25 x 400 x 400 x 4) = 11,463,680,000 bytes = 10.7 GB`. This is the memory required just for the activations and gradients, and does not include the memory required for the weights, biases, or any additional memory required by the training algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806b29f-c075-4019-8e34-2120d2caf6f5",
   "metadata": {},
   "source": [
    "3. If your GPU runs out of memory while training a CNN, what are five things you could try to\n",
    "solve the problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ec559c-f621-4cf0-8320-f50d91ffb59a",
   "metadata": {},
   "source": [
    "If a GPU runs out of memory while training a CNN, there are several things you could try to solve the problem:\n",
    "\n",
    "1. Reduce the batch size: This is the most straightforward solution to reduce the GPU memory usage. Decreasing the batch size will reduce the amount of data that needs to be loaded into the GPU memory during each forward and backward pass.\n",
    "\n",
    "2. Use mixed precision training: Mixed precision training involves using lower precision (e.g. float16) for some computations in the network, which can help reduce the amount of GPU memory required.\n",
    "\n",
    "3. Use gradient checkpointing: Gradient checkpointing involves recomputing some intermediate activations during the backward pass, rather than storing them in memory during the forward pass. This can help reduce the amount of GPU memory required, but may increase the training time.\n",
    "\n",
    "4. Use data parallelism: Data parallelism involves splitting the batch across multiple GPUs, each processing a subset of the data. This can help reduce the memory requirements for each GPU.\n",
    "\n",
    "5. Reduce the model size: If none of the above solutions work, it may be necessary to reduce the size of the model by removing layers or reducing the number of filters in each layer. This can help reduce the amount of memory required to store the model parameters.\n",
    "\n",
    "It's important to note that these solutions are not mutually exclusive and may need to be used in combination to effectively solve the out-of-memory problem. Additionally, some of these solutions may come at the cost of increased training time or decreased accuracy, so it's important to evaluate the trade-offs and choose the best solution based on the specific requirements of the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310aee1-7324-4925-b044-2cf844c4b26b",
   "metadata": {},
   "source": [
    "4. Why would you want to add a max pooling layer rather than a convolutional layer with the\n",
    "same stride?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839f7fe-a7df-4063-b158-3b7b34859ff2",
   "metadata": {},
   "source": [
    "A max pooling layer and a convolutional layer with the same stride can both downsample the input spatially by reducing the resolution of the feature maps. However, there are some reasons why one might want to use a max pooling layer instead of a convolutional layer with the same stride:\n",
    "\n",
    "1. Parameter sharing: A convolutional layer with a large stride can lead to sparse connectivity, where only a subset of the input pixels are used to compute the output. In contrast, max pooling performs parameter sharing by taking the maximum value within each pooling region, which can help reduce overfitting and increase generalization performance.\n",
    "\n",
    "2. Translation invariance: Max pooling can provide some degree of translation invariance, meaning that the output is not sensitive to small translations of the input. This can be beneficial for tasks such as object recognition, where the location of the object in the input can vary.\n",
    "\n",
    "3. Computational efficiency: Max pooling is typically less computationally expensive than a convolutional layer with the same stride, as it requires fewer operations to compute the output. This can help reduce the overall computational cost of the network.\n",
    "\n",
    "However, it's worth noting that max pooling also has some disadvantages compared to using a convolutional layer with a stride, such as the loss of information due to discarding non-maximal values and the inability to learn features that vary in size or shape. As with many design choices in deep learning, the decision to use max pooling versus a convolutional layer with a stride should be based on empirical evaluation and task-specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836d51b-07b9-4c24-89b1-76aae5f63337",
   "metadata": {},
   "source": [
    "5. When would you want to add a local response normalization layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f451e05-c7e7-4b3b-afe6-966c00082aec",
   "metadata": {},
   "source": [
    "A local response normalization (LRN) layer is typically used in convolutional neural networks (CNNs) as a form of local competition normalization. It is intended to increase the generalization performance of the network by normalizing the outputs of neighboring feature maps at the same spatial location. This can help prevent overfitting by reducing the impact of co-adaptation between feature maps.\n",
    "\n",
    "However, the use of LRN layers has become less common in recent years, as they have been found to have limited impact on performance and can even sometimes harm it. In particular, LRN layers tend to be computationally expensive and can slow down training and inference times.\n",
    "\n",
    "Therefore, it is not always necessary to add a LRN layer, and it is largely dependent on the specific requirements of the task at hand. If the task requires high accuracy and the benefits of local normalization outweigh the computational costs, then a LRN layer could be added to the network. Otherwise, it is often omitted in favor of other normalization techniques, such as batch normalization or group normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ebac0-ed44-449f-9f06-17b5c108c857",
   "metadata": {},
   "source": [
    "6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main\n",
    "innovations in GoogLeNet, ResNet, SENet, and Xception?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a814d-62af-492e-a06a-2d2f9465b1c7",
   "metadata": {},
   "source": [
    "Sure, here are the main innovations in each of the mentioned architectures:\n",
    "\n",
    "AlexNet:\n",
    "- Increased model depth and width compared to LeNet-5\n",
    "- Use of ReLU activation function instead of sigmoid\n",
    "- Use of overlapping pooling layers\n",
    "- Use of dropout regularization\n",
    "- Use of data augmentation techniques during training\n",
    "\n",
    "GoogLeNet (Inception-v1):\n",
    "- Use of the Inception module, which includes multiple convolutional filters of different sizes in parallel\n",
    "- Use of 1x1 convolutions to reduce the dimensionality of the feature maps before expensive larger convolutions\n",
    "- Use of global average pooling instead of fully connected layers at the end of the network\n",
    "- Use of auxiliary classifiers to mitigate vanishing gradients during training\n",
    "\n",
    "ResNet:\n",
    "- Use of residual blocks, which include a shortcut connection that bypasses one or more convolutional layers\n",
    "- Use of bottleneck blocks, which use 1x1 convolutions to reduce the dimensionality of the feature maps before expensive larger convolutions\n",
    "- Introduction of very deep architectures (e.g., ResNet-152) with more than 100 layers\n",
    "- Use of skip connections that allow gradients to flow more easily during training\n",
    "\n",
    "SENet:\n",
    "- Use of squeeze-and-excitation blocks, which adaptively recalibrate channel-wise feature responses\n",
    "- Use of global average pooling to reduce the spatial dimensions of the feature maps\n",
    "- Use of learnable parameters to capture channel-wise dependencies\n",
    "\n",
    "Xception:\n",
    "- Use of depthwise separable convolutions, which separate the spatial and channel-wise convolutions\n",
    "- Use of pointwise convolutions (1x1 convolutions) to mix the channels after depthwise separable convolutions\n",
    "- Significantly reduces the number of parameters while maintaining accuracy compared to previous state-of-the-art models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451fd6f-27bc-4e8c-95a0-21eec911dbc1",
   "metadata": {},
   "source": [
    "7. What is a fully convolutional network? How can you convert a dense layer into a\n",
    "convolutional layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbe01b-7c7c-4ad4-8923-80824937bc24",
   "metadata": {},
   "source": [
    "A fully convolutional network (FCN) is a type of neural network architecture commonly used for image segmentation tasks. Unlike traditional convolutional neural networks (CNNs) which use fully connected layers at the end of the network for classification, FCNs use only convolutional layers throughout the network, allowing them to output a dense prediction map for each pixel in the input image.\n",
    "\n",
    "In order to convert a dense layer into a convolutional layer, you can use the following steps:\n",
    "\n",
    "1. Identify the size of the input tensor to the dense layer. This will typically be a flattened vector of some size.\n",
    "\n",
    "2. Determine the number of output units for the dense layer. This will be the number of filters you want to use for the convolutional layer.\n",
    "\n",
    "3. Reshape the output tensor of the previous layer to have the same number of channels as the input tensor to the dense layer, and a width and height of 1. This can be done using a 1x1 convolution with the desired number of output filters.\n",
    "\n",
    "4. Create a convolutional layer with the same number of output filters as the dense layer, using the size of the input tensor as the kernel size and a stride of 1.\n",
    "\n",
    "5. Connect the reshaped tensor from step 3 to the convolutional layer from step 4 as the input tensor.\n",
    "\n",
    "6. Continue the network as desired, using convolutional layers instead of dense layers.\n",
    "\n",
    "This approach effectively converts the dense layer into a 1x1 convolutional layer, allowing it to be trained and used in an FCN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a093f-e723-43fe-a30e-c86c703cc1d8",
   "metadata": {},
   "source": [
    "8. What is the main technical difficulty of semantic segmentation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec0020-de0c-4651-8d1f-8b2c97a861a5",
   "metadata": {},
   "source": [
    "The main technical difficulty of semantic segmentation is achieving high accuracy while also maintaining efficiency and scalability. This is because semantic segmentation requires pixel-wise labeling of images, which can be computationally expensive and memory-intensive, especially for high-resolution images or large datasets. \n",
    "\n",
    "Furthermore, the challenge of semantic segmentation lies in capturing both local and global context information in the image, in order to accurately identify and segment objects of interest. This requires designing a network architecture that can effectively capture both local and global features, while also avoiding overfitting and ensuring efficient use of computational resources.\n",
    "\n",
    "Another difficulty in semantic segmentation is dealing with class imbalance, where certain classes may have significantly fewer pixels than others in the training set. This can lead to biases in the model and reduced accuracy for those underrepresented classes.\n",
    "\n",
    "Finally, semantic segmentation often requires large amounts of labeled data for training, which can be time-consuming and expensive to collect and annotate. Addressing these technical difficulties requires careful consideration of the network architecture, training methodology, and dataset preparation, as well as ongoing research and development in the field of computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad2d36-eb64-447f-b0c7-fe51dae0a65c",
   "metadata": {},
   "source": [
    "9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1f02b-8c5f-4ea9-a4dd-94efdb8e389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 14:24:45.304470: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 14:24:45.606801: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 14:24:45.615504: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 14:24:49.993485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 4s 0us/step\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 63s 33ms/step - loss: 0.1332 - accuracy: 0.9589 - val_loss: 0.0621 - val_accuracy: 0.9809\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0450 - accuracy: 0.9858 - val_loss: 0.0398 - val_accuracy: 0.9868\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0309 - accuracy: 0.9905 - val_loss: 0.0396 - val_accuracy: 0.9869\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 0.0415 - val_accuracy: 0.9871\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 61s 33ms/step - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.0322 - val_accuracy: 0.9889\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0129 - accuracy: 0.9955 - val_loss: 0.0246 - val_accuracy: 0.9919\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0099 - accuracy: 0.9968"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Add a color channel dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73fb53-69e8-44e8-993e-f6d742025d03",
   "metadata": {},
   "source": [
    "10. Use transfer learning for large image classification, going through these steps:\n",
    "\n",
    "a. Create a training set containing at least 100 images per class. For example, you could\n",
    "classify your own pictures based on the location (beach, mountain, city, etc.), or\n",
    "alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "\n",
    "b. Split it into a training set, a validation set, and a test set.\n",
    "\n",
    "c. Build the input pipeline, including the appropriate preprocessing operations, and\n",
    "optionally add data augmentation.\n",
    "\n",
    "d. Fine-tune a pretrained model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05635106-9e9b-4315-96be-529d5c47466f",
   "metadata": {},
   "source": [
    "Sure, here are the steps to perform transfer learning for large image classification:\n",
    "\n",
    "a. Create a training set containing at least 100 images per class: \n",
    "   For example, you can classify your own pictures based on location or use an existing dataset such as CIFAR-10, ImageNet, or TensorFlow Datasets.\n",
    "\n",
    "b. Split the dataset into a training set, a validation set, and a test set: \n",
    "   Typically, the training set should be the largest subset (around 70-80% of the dataset), followed by a validation set (around 10-20%), and finally a test set (10-20%). This split ensures that the model does not overfit on the training set and can generalize well to unseen data.\n",
    "\n",
    "c. Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation:\n",
    "   Use TensorFlow's Data API to read in the data and preprocess it. Preprocessing operations can include resizing, normalizing, and data augmentation (such as random cropping, flipping, or rotating images). Data augmentation is particularly important for improving the model's ability to generalize to new data.\n",
    "\n",
    "d. Fine-tune a pretrained model on this dataset:\n",
    "   Choose a pre-trained model such as VGG16, ResNet50, or InceptionV3 and remove the top layers. Replace the top layers with new layers that are suited for your classification task (e.g., a dense layer with a softmax activation function for classification). Freeze the lower layers of the model and fine-tune the top layers on the new dataset. Train the model using a suitable optimizer such as Adam or SGD, and monitor the performance on the validation set. Finally, evaluate the performance of the model on the test set.\n",
    "\n",
    "With these steps, you should be able to perform transfer learning for large image classification successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab107a77-361b-4a64-8039-022b72336dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb13e65-f25f-4f7b-a848-77703c656db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
