{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d376526b-aab1-47ef-a442-08b197627658",
   "metadata": {},
   "source": [
    "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1e866-a95a-47c2-b9ec-8a0f10f2de3e",
   "metadata": {},
   "source": [
    "The sequence modeling algorithum hence come in two flavors, Stateless and Stateful, depending upon the architecture \n",
    "used while training.Following is a discussion using LSTM as an example, but the notion is application to other \n",
    "variants as well, namely RNN, GRU, etc.\n",
    "\n",
    "This architecture is used when the IID assumption holds.While creating batches for training, this means that there\n",
    "is no inter-relationship across the batches, and each batch is independent of one other.\n",
    "\n",
    "The way these two architectures differ is the manner in which the states (cell and hidden states) of the model\n",
    "(corresponding to each batch) are initialized as the training progressses from one batch to another. This is not to \n",
    "be confused with the parameters/weights, which are anyways propagated through the entire training process (Which is \n",
    "the whole point of training )\n",
    "The initial states of LSTM are reset to zeros every time the new batch is taken up and processed, thus not utilizing \n",
    "the already learned internal activations (states).This forces the model to forget the learnings from previous\n",
    "batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171093e-e0f0-4012-bf54-28cd9d4cf6ae",
   "metadata": {},
   "source": [
    "2. Why do people use Encoderâ€“Decoder RNNs rather than plain sequence-to-sequence RNNs\n",
    "for automatic translation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f5717-8a0f-43e9-b9f8-2dccdcd20b26",
   "metadata": {},
   "source": [
    "Seq-2-seq RNNs translate one word at a time. encoder-decoder RNNs read & translate a sentence at a time.\n",
    "The encoder would convert this sentence into a single vector representation, and then the decoder would decode this vector into a sentence in \n",
    "another language. This two-step model, called an Encoder Decoder, works much better than trying to translate on the fly with a single sequence-\n",
    "to-sequence RNN (like the one represented on the top left), since the last words of a sentence can affect the first words of the translation, so \n",
    "you need to wait until you have heard the whole sentence before translation it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4580d7-4b9d-4ceb-baf7-259ae528ac5f",
   "metadata": {},
   "source": [
    "3. How can you deal with variable-length input sequences? What about variable-length output\n",
    "sequences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51abe9-0dd8-4d5d-aa78-b9001bf8596a",
   "metadata": {},
   "source": [
    "RNNs, or recurrent neural networks, can perform tasks on word, sentence, paragraph , or even document length input.However, this invites a couple of\n",
    "challenges involving the length of the input :\n",
    "    \n",
    "    1.how to handle short and variable-length inputs (e.g. sentence of different lengths) \n",
    "    2. what to do with very long inputs (which are computationally intensive ).\n",
    "\n",
    "Naive approaches :\n",
    "\n",
    "    The simplest approach to long sequence is to simply truncate them, usually at the end but potentially at the begining. This represents data loss, but might not be all that bad.\n",
    "\n",
    "Truncated backpropogation:\n",
    "\n",
    "    \"Truncated backpropagation through time\" is a complicated name for a simple idea that is relatively in vogue right now:performing gradient updates based on backpropogation passes through a tail-end subset of the LSTM layers. This coarse approach results in less accurate updates, especially to the earlier LSTM time-step layers, but also makes for much faster trained times.\n",
    "\n",
    "Encoder-decoders:\n",
    "\n",
    "    Autoencoders can be used to compress the sentence to a smaller space. This would essentially involve training an autoencoder, then feeding the input to just the first half of the model, the constraining half. The output vectors will be mathematically compressed word embeddings according to some loss function, so they will no longer be mappable directly back to the sequence they came from.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653be49-13f8-434d-92e2-d45f4f353560",
   "metadata": {},
   "source": [
    "4. What is beam search and why would you use it? What tool can you use to implement it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780eba9b-5f6c-423b-9efa-25a39e32f50d",
   "metadata": {},
   "source": [
    "Beam search is the most popular search strategy for the sequence to sequence Deep NlP algorithum like Neural Machine \n",
    "Translation, Image captioning, Chatbots, etc. Beam search considers muliple best options based on beamwidth using \n",
    "conditional probability, which is better than the sub-optimal Greedy search.\n",
    "The greedy search and beam search decoding algorithums that can be used on text generation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a6f227-0d0d-49ff-b256-efafc1ae5446",
   "metadata": {},
   "source": [
    "5. What is an attention mechanism? How does it help?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc7b16b-0bdc-4fb4-a50c-71fa6f61621b",
   "metadata": {},
   "source": [
    "The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input \n",
    "sequence in a flexible manner, by a weighted combination  of all the encoded input vectors, with the most relevant vectors \n",
    "being attributed the hightest weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91769c-a11f-4ace-8a00-7d0057a8a08e",
   "metadata": {},
   "source": [
    "6. What is the most important layer in the Transformer architecture? What is its purpose?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d60605-91ea-4ef2-af96-04326fe3e9e2",
   "metadata": {},
   "source": [
    "The most important part here is the \"Residual connections\" around the layers. This is very important in retaining \n",
    "the position related information which we are adding to the input representation/embedding across the network. The network\n",
    "displayed catastrophic results on removing the Residual Connections.\n",
    "Residual connection provides another path for data to reach latter parts of the neurals network by skipping some layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9636f9-68db-401b-b4c6-7ae734129f0a",
   "metadata": {},
   "source": [
    "7. When would you need to use sampled softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd0ef7-c750-45d4-937c-82ba4587f06e",
   "metadata": {},
   "source": [
    "I'd probably consider using sampled softmax if I have over 100,000 classes, or if my final classification layer\n",
    "dominates overall execution time or memory use. An obvious application is large word voacbularies, for example in language\n",
    "modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3d497-af30-48e6-b33a-0d3eda36ceae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a72f1c-b42a-4d7f-81f1-62185866c123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634c210-50cc-49b2-8945-ca9465da1267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
