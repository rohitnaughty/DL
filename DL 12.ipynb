{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3137749c-6f6a-47da-99b3-5cc51e03d1dd",
   "metadata": {},
   "source": [
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91fda5-cb6e-483c-aeff-6c2698c3861b",
   "metadata": {},
   "source": [
    "\n",
    "The unsqueeze() function in PyTorch and other similar libraries helps to increase the dimensions of a tensor by inserting one or more dimensions of size 1 at the specified position(s). This can be particularly useful in broadcasting operations, where we need to perform element-wise operations between tensors that might have different dimensions.\n",
    "\n",
    "For example, let's say we have a 2D tensor A with shape (3, 4) and a 1D tensor B with shape (4,). If we want to perform element-wise multiplication between A and B, we can't do it directly because their shapes are not compatible for broadcasting. However, we can use unsqueeze() to add an extra dimension to B so that its shape becomes (1, 4), which is compatible with the second dimension of A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edcb5cc-11e8-4f2e-bad4-be9635e65ce8",
   "metadata": {},
   "source": [
    "2. How can we use indexing to do the same operation as unsqueeze?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67397a03-43a8-4ea8-944e-b56870188927",
   "metadata": {},
   "source": [
    "We can use indexing to achieve the same result as unsqueeze() by manually adding new dimensions to a tensor.\n",
    "\n",
    "To add a new dimension of size 1 at a specific position in a tensor, we can use the None or np.newaxis keyword along that dimension's axis. For example, let's say we have a 1D tensor A with shape (3,). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad2d98-e9ce-45f2-952e-07c3a8aa7842",
   "metadata": {},
   "source": [
    "3. How do we show the actual contents of the memory used for a tensor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1279ddd-9486-49e4-b921-e6932b6a831d",
   "metadata": {},
   "source": [
    "\n",
    "To show the actual contents of the memory used for a tensor in PyTorch, we can use the numpy() method to convert the tensor to a NumPy array, and then use the data attribute of the NumPy array to access the underlying memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d0431-a444-4d48-aa06-935f483ada48",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
    "code in a notebook.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20dd1a6-574d-4e25-b085-17b46a783663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.7)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.0)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.2.tar.gz (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.2-py3-none-any.whl size=88177 sha256=99e46c80a0a7aa03694713fca27991449c39f2621342ec42348e5fdf088c798d\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e2/ba/ec/aad60e5609dfac997d2902a02912124867d56a41334570a4e4\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.3 filelock-3.12.0 lit-16.0.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67962edc-6174-49c9-abbe-e43f629d0146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 22, 33],\n",
      "        [14, 25, 36],\n",
      "        [17, 28, 39]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a matrix and a vector\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = torch.tensor([10, 20, 30])\n",
    "\n",
    "# Add the vector to the matrix\n",
    "C = A + B.unsqueeze(0)\n",
    "\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647bec1f-b242-41dc-a167-006375675bf4",
   "metadata": {},
   "source": [
    "In this example, B.unsqueeze(0) adds a new dimension of size 1 at position 0 of the vector B, so that it has shape 1×3. When we add this vector to the matrix A, PyTorch broadcasts the vector along the rows of the matrix, creating a temporary matrix of shape 3×3 with the same elements as the vector. Then, it performs element-wise addition between the temporary matrix and A to produce the result C, which is also a matrix of size 3×3.\n",
    "\n",
    "Note that if we wanted to add the vector to each column of the matrix instead, we would need to transpose the matrix, add the vector to each row of the transposed matrix, and then transpose the result back to the original shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081cbcd3-3ee3-40b8-9b0b-30e48ea22702",
   "metadata": {},
   "source": [
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d3b1b-f299-4280-95e8-b629fa1ec9ee",
   "metadata": {},
   "source": [
    "Broadcasting and `expand_as` do not necessarily result in increased memory use, as they do not create new copies of the data. \n",
    "\n",
    "Broadcasting is a way to perform operations between tensors of different shapes by implicitly replicating the smaller tensor to match the shape of the larger tensor. This replication is done on the fly and does not create new copies of the data, so it does not increase memory use. The replicated tensor is treated as if it were a view of the original tensor, so any changes made to the replicated tensor are reflected in the original tensor.\n",
    "\n",
    "Similarly, `expand_as` is a method that returns a new tensor with the same data as the original tensor, but with additional dimensions added and existing dimensions replicated as necessary to match the shape of the specified tensor. Like broadcasting, `expand_as` does not create new copies of the data, so it does not increase memory use. The new tensor returned by `expand_as` is a view of the original tensor, so any changes made to the new tensor are reflected in the original tensor.\n",
    "\n",
    "However, it's worth noting that while broadcasting and `expand_as` do not create new copies of the data, they may still result in increased memory use if the resulting tensor is significantly larger than the original tensor. This is because the larger tensor may require more memory to store than the original tensor, even though the data is not duplicated. In addition, if the operations performed on the tensors are memory-intensive (such as matrix multiplication), the increased size of the resulting tensor may lead to higher memory usage during the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb84a4-ea5b-4f4a-b346-69e96fbc33a2",
   "metadata": {},
   "source": [
    "6. Implement matmul using Einstein summation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f880f793-80eb-435c-b7e0-8765bc2d1f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 58,  64],\n",
      "        [139, 154]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create two matrices\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "B = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
    "\n",
    "# Compute matrix product using Einstein summation\n",
    "C = torch.einsum('ij,jk->ik', A, B)\n",
    "\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78404ac-b0d3-48e7-947f-3a351957f5be",
   "metadata": {},
   "source": [
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b81d9-be38-4b98-9a99-110e3f83c1ba",
   "metadata": {},
   "source": [
    "When a repeated index letter appears on the left-hand side of an Einstein summation notation string in `einsum`, it means that the corresponding dimension of the input tensor should be summed over. \n",
    "\n",
    "For example, consider the string `'ii->i'`. This notation specifies that we want to sum over the first index of a 2D input tensor with shape `(n, n)`, and return a 1D output tensor with shape `(n,)`. The repeated index letter `i` indicates that we want to sum over the elements along the diagonal of the matrix. The resulting output tensor will contain the diagonal elements of the input matrix.\n",
    "\n",
    "Similarly, consider the string `'ijk->jik'`. This notation specifies that we want to transpose the first and second dimensions of a 3D input tensor with shape `(n, m, p)`. The repeated index letter `i` indicates that we want to keep the elements along the third dimension fixed and swap the first and second dimensions. The resulting output tensor will have shape `(m, n, p)`, with the first and second dimensions transposed.\n",
    "\n",
    "Note that in the output string of `einsum`, each index letter must appear exactly once. The repeated index letters on the left-hand side specify which dimensions to sum over or transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e857d-c3f1-45b0-8f55-559289284534",
   "metadata": {},
   "source": [
    "8. What are the three rules of Einstein summation notation? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8183a-bb36-42ea-b70e-155d4ac60d3b",
   "metadata": {},
   "source": [
    "The three rules of Einstein summation notation are:\n",
    "\n",
    "1. Repeated indices are summed over. \n",
    "\n",
    "2. Each index appears only twice in a term, once as a subscript and once as a superscript. \n",
    "\n",
    "3. The order of the terms does not matter.\n",
    "\n",
    "These rules were developed by Albert Einstein to simplify the notation of tensor calculus. They allow complex tensor operations to be expressed using simple algebraic expressions, and they make it easier to perform tensor manipulations and transformations.\n",
    "\n",
    "The first rule states that repeated indices in a term are implicitly summed over. This means that if an index appears twice in a term (once as a subscript and once as a superscript), then the tensor product represented by that term should be summed over that index. This is equivalent to performing a dot product, or matrix multiplication, between the corresponding rows and columns of the tensors.\n",
    "\n",
    "The second rule ensures that the index notation is unambiguous and consistent. Each index appears exactly twice in a term, once as a subscript and once as a superscript. This allows the terms to be combined and simplified in a straightforward manner.\n",
    "\n",
    "The third rule states that the order of the terms does not matter. This means that the order in which the terms are written does not affect the final result, as long as the indices are properly matched. This is useful when performing tensor manipulations that involve multiple terms, as it allows the terms to be rearranged and combined in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d50ae-481b-4b66-a710-6623e6dd1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What are the forward pass and backward pass of a neural network?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2646809-6f29-427a-ac66-61068881b394",
   "metadata": {},
   "source": [
    "The forward pass and backward pass are the two main steps of training a neural network using backpropagation. Here's a brief explanation of each step:\n",
    "\n",
    "1. Forward pass: During the forward pass, the input data is passed through the neural network layer by layer, and the output of each layer is computed. The output of the final layer is compared to the ground truth labels to compute the loss function, which measures how well the network is performing on the given task.\n",
    "\n",
    "2. Backward pass: During the backward pass, the gradients of the loss function with respect to the parameters of the network are computed using the chain rule of differentiation. The gradients are then used to update the parameters of the network, typically using an optimization algorithm such as stochastic gradient descent. This step is known as backpropagation.\n",
    "\n",
    "The backward pass is crucial for training a neural network, as it allows the network to adjust its parameters to minimize the loss function and improve its performance on the given task. The forward pass and backward pass are typically repeated multiple times (or \"epochs\") until the network converges to a satisfactory solution.\n",
    "\n",
    "It's worth noting that the forward pass and backward pass can also be performed together in a single step, using automatic differentiation libraries such as PyTorch or TensorFlow. In this case, the gradients are computed automatically using the backpropagation algorithm, without the need for manual implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840ec0d-0c3b-45d4-8b9b-5e4c42b47074",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
    "forward pass?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96355465-c09c-4553-8568-b62565633077",
   "metadata": {},
   "outputs": [],
   "source": [
    "Storing activations calculated for intermediate layers in the forward pass is important for several reasons:\n",
    "\n",
    "1. Backpropagation: During the backward pass, we need to compute the gradients of the loss function with respect to the parameters of the network. To do this, we need to propagate the gradients backwards through the layers of the network using the chain rule of differentiation. This requires the intermediate activations to be available, as they are needed to compute the gradients.\n",
    "\n",
    "2. Reuse of activations: In some cases, we may need to reuse the activations of intermediate layers for other purposes. For example, we might want to extract features from the activations of a pre-trained neural network for use in a different model or application.\n",
    "\n",
    "3. Debugging: Storing the activations of intermediate layers can be useful for debugging and visualizing the behavior of the network. By inspecting the activations at different layers, we can gain insight into how the network is processing the input data and how the features are being transformed and combined.\n",
    "\n",
    "4. Efficient computation: In some cases, it may be more efficient to compute the activations of intermediate layers once and reuse them multiple times, rather than recalculating them for each new input. This can be particularly useful for large or complex networks that require a lot of computation.\n",
    "\n",
    "Overall, storing activations of intermediate layers in the forward pass is an important part of training and using neural networks, as it allows us to perform efficient and effective computations, and gain insight into the behavior of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac15a38-77af-409f-9e97-00b63ae87e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487c75a-45a5-41df-adfa-9ffaa4e9f907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc33eb-1d90-464f-9191-ff739150f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ed10b-455b-4753-b993-cfbc96da8f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0475973-b631-4c50-81bc-eb00e9b22d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2122bb4-3eb9-45a6-a5f0-fe7852b4c935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
